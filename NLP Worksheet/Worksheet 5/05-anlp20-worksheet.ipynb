{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ANLP 2020/21; Uni Potsdam; D. Schlangen, B. Aktas_\n",
    "\n",
    "\n",
    "# Work Sheet for Week 05: Neural Networks & Neural Language Models\n",
    "-------\n",
    "\n",
    "## Background\n",
    "\n",
    "In this worksheet, we will follow the material covered in [Chapter 7 of the JM book](https://web.stanford.edu/~jurafsky/slp3/7.pdf). Also especially for Pytorch examples we'll refer to external resources that are given in the question definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E1] We know that in neural networks, given a set of inputs $x_{1}$,$x_{2}$,...,$x_{n}$, a neural unit yields an output z, which can be described as the following weighted sum:**\n",
    "\\begin{equation*}\n",
    "z = (\\sum_{i=1}^{n}w_ix_i) + b = w \\cdot x + b \\end{equation*} <br>\n",
    "\n",
    "**However neural units apply one more operation to this weighted sum ( _z_ ): a non-linear function _f_ that maps _z_ to the activation value _a_ of a given neural unit:**\n",
    "\n",
    "**\\begin{equation*}\n",
    "a = f(z) \\end{equation*} <br>**\n",
    "\n",
    "**Before the introduction of neural networks, we have already seen such non-linear functions, such as the sigmoid function:**\n",
    "\n",
    "**\\begin{equation*}\n",
    "\\sigma (z) = \\frac{1}{1+e^{-z}} \\end{equation*} <br>**\n",
    "\n",
    "**Now can you talk about other non-linear activation functions that are typically used in neural networks? Could you write down the formulas as functions mapping z to a? Finally, considering the varying shapes of such activation functions, how do their properties differ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E2] Having already learned how logistic regression works earlier, how can we relate the working mechanism of feedforward neural networks to that of logistic regression? What are some common and different characteristics?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E3] In real life neural networks are mostly implemented by the help of many different open source libraries such as Keras or Pytorch. Below, try to implement your own Feed Forward Neural Network by using Pytorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an example, the script below produces a Perceptron class, in which we can create a Perceptron object with 5 input nodes and 1 output node. The non-linear activation function is set to be ReLU. Read the script and try to get familier with the structure that uses the <mark>torch.nn.Module</mark> class as a subclass. Refer to [the class documentation](https://pytorch.org/docs/stable/nn.html) for understanding the methods and building blocks used in this class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(5,1) # .fc stands for fully connected. \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x): # x will be a placeholder for the model input\n",
    "        output = self.fc(x)\n",
    "        output = self.relu(x) \n",
    "        return output\n",
    "\n",
    "Perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, create your own Feedforward object by using the script template below. This time, try the following:**\n",
    "- Create the class so that the class object would be a FFNN with 1 input, 1 output and 1 hidden layers.\n",
    "- Instead of having fixed layer sizes, pass layer sizes as arguments to the $__init__$ method of the class.\n",
    "- Define two different fully connected layers, one from input layer to hidden layer, and one from hidden layer to output layer.\n",
    "- This time, let tanh be the activation function that will be applied to the hidden units, and Sigmoid to the output units.\n",
    "- Print a class object with 10 input nodes, 5 hidden nodes and 2 output nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Feedforward(torch.nn.Module):\n",
    "    '''Your code will be here'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4] Now that we have familiarity with Pytorch, we'll try to implement our own FFNN Language Model to train our own embeddings. We'll need a text to train our embedding matrix, a model architecture in Pytorch, and a training loop with optimizers and loss calculation. Please refer to [this source](https://cs230.stanford.edu/blog/pytorch/) to get to know how to train a simple model with Pytorch. We'll use a language model that takes the embeddings of the previous 3 words in a text to guess the next word, so that the probability distribution of that next word can be appromixated by: <br><br>\n",
    "\\begin{equation*}\n",
    "P(w_n| w_1^{n-1}) \\approx P(w_n| w_{n-1}, w_{n-2}, w_{n-3}) \\end{equation*} <br> with the exception that unline n-gram models, we'll use word embeddings instead of actual words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4a] Use the following sentence taken as an excerpt from [this Wikipedia article](https://en.wikipedia.org/wiki/Word_embedding) that talks about the limitations of word embeddings in general. As a first step, write a script that:<br>**\n",
    "- tokenizes the sentence (using nltk <mark>word_tokenize</mark>) and saves the tokenized version in a variable\n",
    "- creates another variable for the vocabulary, as a set of unique tokens in our sentence (for simplicity, without dealing with capitalization)\n",
    "- creates a dictionary that maps each unique word in the vocabulary to an integer id.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = \"One of the main limitations of word embeddings (word vector space models in general) is that \\\n",
    "words with multiple meanings are conflated into a single representation (a single vector in the semantic space). \\\n",
    "In other words, polysemy and homonymy are not handled properly. For example, in the sentence 'The club I tried \\\n",
    "yesterday was great!', it is not clear if the term club is related to the word sense of a club sandwich, baseball \\\n",
    "club, clubhouse, golf club, or any other sense that club might have. The necessity to accommodate multiple meanings \\\n",
    "per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split \\\n",
    "single-sense embeddings into multi-sense ones. Most approaches that produce multi-sense embeddings can be divided \\\n",
    "into two main categories for their word sense representation, i.e., unsupervised and knowledge-based. Based on \\\n",
    "word2vec skip-gram, Multi-Sense Skip-Gram (MSSG) performs word-sense discrimination and embedding simultaneously,\\\n",
    "improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric \\\n",
    "Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of \\\n",
    "lexical databases (e.g., WordNet, ConceptNet, BabelNet), word embeddings and word sense disambiguation, \\\n",
    "Most Suitable Sense Annotation (MSSA) labels word-senses through an unsupervised and knowledge-based approach \\\n",
    "considering a wordâ€™s context in a pre-defined sliding window. Once the words are disambiguated, they can be used in \\\n",
    "a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the \\\n",
    "disambiguation and annotation process to be performed recurrently in a self-improving manner. The use of multi-sense \\\n",
    "embeddings is known to improve performance in several NLP tasks, such as part-of-speech tagging, semantic relation \\\n",
    "identification, and semantic relatedness. However, tasks involving named entity recognition and sentiment analysis \\\n",
    "seem not to benefit from a multiple vector representation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "'''Your code will be here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4b] Write a class named 'TwoLayerFFNNEmbeddings' that will have <mark>$__init__$</mark>  and <mark>forward</mark> methods:**<br>\n",
    "\n",
    "**Its <mark>$__init__$</mark> method:**<br>\n",
    "- takes the vocabulary size and embedding dimension as two arguments\n",
    "- initializes an embedding tensor using the arguments vocabulary size and embedding dimension. Please use the [documentation of nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) to get to know how to create an Embedding tensor, with dimensions (size of the vocabulary x the dimension of each vector embedding).\n",
    "- initializes two consequtive linear layers: first one taking the embedding vectors of 3 previous tokens as its input, and connecting it to the hidden layer of 128 units; second one connecting these 128 hidden nodes to the output layer of |Vocabulary| size.<br>\n",
    "\n",
    "**Its <mark>forward</mark> method:**<br>\n",
    "-  uses a rehaped version of the embeddings created in <mark>$__init__$</mark> (this line of code is given to you, you need to change the variable name for the embeddings. check [here](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) to see what the view method actually does to the tensor).\n",
    "- applied ReLU activation function to the first layer, using the torch.nn.functional package.\n",
    "- returns the log probability tensor of the output layer, by applying the 'log_softmax' function that is found again in the torch.nn.functional package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerFFNNEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        '''Your code will be here'''\n",
    "    def forward(self, X):\n",
    "        '''Your code will be here'''\n",
    "        reshaped_embeddings = self.embeddings(X).view((1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4c] Now, first create your model with the vocabulary size and a embedding dimension of 20. Also choose your favorite loss function and optimizer, and save all to variables. Then using these variables, write a function that will loop over the tokenized dataset and:**<br>\n",
    "- put the indices of the embedding vectors of the following 3 tokens into the <mark>forward</mark> method and calculate log probabilities\n",
    "- calculate the loss by using the log probabilities acquired in the previous step and target word at each iteration, given by your desired loss function\n",
    "- compute the gradient by the backward pass and update the gradient\n",
    "- add the resulting loss to the cumulative loss <br>\n",
    "\n",
    "**in each iteration. In the end run your function to print out the total loss reached by iterating over the whole dataset just once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '''Your code will be here'''\n",
    "loss_fn = '''Your code will be here'''\n",
    "optimizer = '''Your code will be here'''\n",
    "\n",
    "def loss_calculator():\n",
    "    cumulative_loss = 0\n",
    "    for i in range(len(tokenized)-4):\n",
    "        model.zero_grad() # this line of code will refresh the gradients at each step, please keep it!\n",
    "    '''Your code will be here'''\n",
    "    \n",
    "    return cumulative_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4d] As a final step, write a small script that only runs your loss_calculator function ten consecutive times and prints out the loss. Comment on how the loss value changes at each iteration. What does it signify in terms of model learning?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Your code will be here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we carry out iterations over the whole dataset, the model keeps the learned weights in the embedding matrix and uses that in the next iteration. Therefore it develops an understanding to guess the next word better, judging from the embeddings of the context words coming before that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
