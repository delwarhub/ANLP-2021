{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ANLP 2020/21; Uni Potsdam; D. Schlangen, B. Aktas_\n",
    "\n",
    "\n",
    "# Work Sheet for Week 05: Neural Networks & Neural Language Models\n",
    "-------\n",
    "\n",
    "## Background\n",
    "\n",
    "In this worksheet, we will follow the material covered in [Chapter 7 of the JM book](https://web.stanford.edu/~jurafsky/slp3/7.pdf). Also especially for Pytorch examples we'll refer to external resources that are given in the question definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E1] We know that in neural networks, given a set of inputs $x_{1}$,$x_{2}$,...,$x_{n}$, a neural unit yields an output z, which can be described as the following weighted sum:**\n",
    "\\begin{equation*}\n",
    "z = (\\sum_{i=1}^{n}w_ix_i) + b = w \\cdot x + b \\end{equation*} <br>\n",
    "\n",
    "**However neural units apply one more operation to this weighted sum ( _z_ ): a non-linear function _f_ that maps _z_ to the activation value _a_ of a given neural unit:**\n",
    "\n",
    "**\\begin{equation*}\n",
    "a = f(z) \\end{equation*} <br>**\n",
    "\n",
    "**Before the introduction of neural networks, we have already seen such non-linear functions, such as the sigmoid function:**\n",
    "\n",
    "**\\begin{equation*}\n",
    "\\sigma (z) = \\frac{1}{1+e^{-z}} \\end{equation*} <br>**\n",
    "\n",
    "**Now can you talk about other non-linear activation functions that are typically used in neural networks? Could you write down the formulas as functions mapping z to a? Finally, considering the varying shapes of such activation functions, how do their properties differ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Answer E1]** \n",
    "\n",
    "In chapter 7 of the book, three different non-linear activation functions are introduced: <br>\n",
    "\n",
    "Sigmoid: \n",
    "\\begin{equation*}\n",
    "a = \\sigma (z) = \\frac{1}{1+e^{-z}} \\end{equation*} <br>\n",
    "\n",
    "\n",
    "tanh: \n",
    "\\begin{equation*}\n",
    "a = tanh (z) = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\end{equation*} <br>\n",
    "\n",
    "ReLU:\n",
    "\\begin{equation*}\n",
    "a = max(z,0) \\end{equation*} <br>\n",
    "\n",
    "In Sigmoid and tanh, the tails of the function go almost paralel to the horizontal axis. Therefore for large values of _z_ in both negative and positive direction, _f(z)_ will barely react. Consequently , we'll get an almost zero slope for such large z values, impeding our learning rate significantly. Nevertheless, both functions are quite responsive to rather small values of _z_, and this will result in fast learning with high derivatives as a response to small changes in _z_. \n",
    "\n",
    "In ReLU, when _z_ is negative, we will have 0 as our activation value. This will cause the learning to stop, without any slope, gradient will not be calculated. In the positive axes, ReLU will allow smooth derivation just like a linear function. And when combined with many neural units, the result will be a non-linear function since ReLU is a non-linear function itself, although acting similarly to linear functions.\n",
    "\n",
    "For examples outside of the scope of chapter 7 of the book, [this link](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a) provides useful explanations and a variety of other activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E2] Having already learned how logistic regression works earlier, how can we relate the working mechanism of feedforward neural networks to that of logistic regression? What are some common and different characteristics?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Answer E2]** \n",
    "\n",
    "In both cases, we first consider an input vector _x_, multiplied by the weight matrix _W_, and the bias term _b_ added. The resulting value is applied a non-linear (activation) function to be mapped to a real-valued intermediate output (before being mapped by softmax to a value between 0 and 1 to represent a probability).  <br>\n",
    "\n",
    "At this point, imagine having n many such intermediate outputs. This is the equivalent of having a hidden layer with n unit in a neural network. In logistic regression, we simply work with a single such unit. In other words, logistic regression is similar to a FFNN without any hidden layers, thus a 1-layer FFNN. As a matter of fact, in neural networks we would need another weight matrix, this time to be applied to those n units in the first hidden layer. Therefore every hidden layer might be considered as the 'input layer' for the one coming after it, and 'output layer' for the one coming before it. Eventually, regardless of having a neural network or a regression model, we will have to map the value(s) of the node(s) in the final layer of our model to a probability distribution representing each possible output label. <br>\n",
    "\n",
    "Another important distinction is, each unit of each hidden layer in FFNNs can be thought of feature representations of the outputs of the previous layer. Therefore we don't need to introduce feature templates for feature representations in neural networks. <br>\n",
    "\n",
    "It is evident that the use of non-linear functions and softmax can be regarded as commonalities of both model types, apart from the obvious structural similarities explained thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E3] In real life neural networks are mostly implemented by the help of many different open source libraries such as Keras or Pytorch. Below, try to implement your own Feed Forward Neural Network by using Pytorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an example, the script below produces a Perceptron class, in which we can create a Perceptron object with 5 input nodes and 1 output node. The non-linear activation function is set to be ReLU. Read the script and try to get familier with the structure that uses the <mark>torch.nn.Module</mark> class as a subclass. Refer to [the class documentation](https://pytorch.org/docs/stable/nn.html) for understanding the methods and building blocks used in this class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(\n",
       "  (fc): Linear(in_features=5, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(5,1) # .fc stands for fully connected. \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x): # x will be a placeholder for the model input\n",
    "        output = self.fc(x)\n",
    "        output = self.relu(output) \n",
    "        return output\n",
    "\n",
    "Perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, create your own Feedforward object by using the script template below. This time, try the following:**\n",
    "- Create the class so that the class object would be a FFNN with 1 input, 1 output and 1 hidden layers.\n",
    "- Instead of having fixed layer sizes, pass layer sizes as arguments to the $__init__$ method of the class.\n",
    "- Define two different fully connected layers, one from input layer to hidden layer, and one from hidden layer to output layer.\n",
    "- This time, let tanh be the activation function that will be applied to the hidden units, and Sigmoid to the output units.\n",
    "- Print a class object with 10 input nodes, 5 hidden nodes and 2 output nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Feedforward(torch.nn.Module):\n",
    "    '''Your code will be here'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Answer E3]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedforward(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class Feedforward(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.output_size = output_size\n",
    "            self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.fc2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            tanh = self.tanh(hidden)\n",
    "            output = self.fc2(tanh)\n",
    "            output = self.sigmoid(output)\n",
    "            return output\n",
    "Feedforward(10,5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4] Now that we have familiarity with Pytorch, we'll try to implement our own FFNN Language Model to train our own embeddings. We'll need a text to train our embedding matrix, a model architecture in Pytorch, and a training loop with optimizers and loss calculation. Please refer to [this source](https://cs230.stanford.edu/blog/pytorch/) to get to know how to train a simple model with Pytorch. We'll use a language model that takes the embeddings of the previous 3 words in a text to guess the next word, so that the probability distribution of that next word can be appromixated by: <br><br>\n",
    "\\begin{equation*}\n",
    "P(w_n| w_1^{n-1}) \\approx P(w_n| w_{n-1}, w_{n-2}, w_{n-3}) \\end{equation*} <br> with the exception that unline n-gram models, we'll use word embeddings instead of actual words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4a] Use the following sentence taken as an excerpt from [this Wikipedia article](https://en.wikipedia.org/wiki/Word_embedding) that talks about the limitations of word embeddings in general. As a first step, write a script that:<br>**\n",
    "- tokenizes the sentence (using nltk <mark>word_tokenize</mark>) and saves the tokenized version in a variable\n",
    "- creates another variable for the vocabulary, as a set of unique tokens in our sentence (for simplicity, without dealing with capitalization)\n",
    "- creates a dictionary that maps each unique word in the vocabulary to an integer id.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = \"One of the main limitations of word embeddings (word vector space models in general) is that \\\n",
    "words with multiple meanings are conflated into a single representation (a single vector in the semantic space). \\\n",
    "In other words, polysemy and homonymy are not handled properly. For example, in the sentence 'The club I tried \\\n",
    "yesterday was great!', it is not clear if the term club is related to the word sense of a club sandwich, baseball \\\n",
    "club, clubhouse, golf club, or any other sense that club might have. The necessity to accommodate multiple meanings \\\n",
    "per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split \\\n",
    "single-sense embeddings into multi-sense ones. Most approaches that produce multi-sense embeddings can be divided \\\n",
    "into two main categories for their word sense representation, i.e., unsupervised and knowledge-based. Based on \\\n",
    "word2vec skip-gram, Multi-Sense Skip-Gram (MSSG) performs word-sense discrimination and embedding simultaneously,\\\n",
    "improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric \\\n",
    "Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of \\\n",
    "lexical databases (e.g., WordNet, ConceptNet, BabelNet), word embeddings and word sense disambiguation, \\\n",
    "Most Suitable Sense Annotation (MSSA) labels word-senses through an unsupervised and knowledge-based approach \\\n",
    "considering a word’s context in a pre-defined sliding window. Once the words are disambiguated, they can be used in \\\n",
    "a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the \\\n",
    "disambiguation and annotation process to be performed recurrently in a self-improving manner. The use of multi-sense \\\n",
    "embeddings is known to improve performance in several NLP tasks, such as part-of-speech tagging, semantic relation \\\n",
    "identification, and semantic relatedness. However, tasks involving named entity recognition and sentiment analysis \\\n",
    "seem not to benefit from a multiple vector representation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "'''Your code will be here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[AnswerE4a]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = word_tokenize(train_sentence)\n",
    "vocabulary = set(tokenized)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4b] Write a class named 'TwoLayerFFNNEmbeddings' that will have <mark>$__init__$</mark>  and <mark>forward</mark> methods:**<br>\n",
    "\n",
    "**Its <mark>$__init__$</mark> method:**<br>\n",
    "- takes the vocabulary size and embedding dimension as two arguments\n",
    "- initializes an embedding tensor using the arguments vocabulary size and embedding dimension. Please use the [documentation of nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) to get to know how to create an Embedding tensor, with dimensions (size of the vocabulary x the dimension of each vector embedding).\n",
    "- initializes two consequtive linear layers: first one taking the embedding vectors of 3 previous tokens as its input, and connecting it to the hidden layer of 128 units; second one connecting these 128 hidden nodes to the output layer of |Vocabulary| size.<br>\n",
    "\n",
    "**Its <mark>forward</mark> method:**<br>\n",
    "-  uses a rehaped version of the embeddings created in <mark>$__init__$</mark> (this line of code is given to you, you need to change the variable name for the embeddings. check [here](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) to see what the view method actually does to the tensor).\n",
    "- applied ReLU activation function to the first layer, using the torch.nn.functional package.\n",
    "- returns the log probability tensor of the output layer, by applying the 'log_softmax' function that is found again in the torch.nn.functional package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerFFNNEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        '''Your code will be here'''\n",
    "    def forward(self, X):\n",
    "        '''Your code will be here'''\n",
    "        reshaped_embeddings = self.embeddings(X).view((1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[AnswerE4b]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4833, -1.2072, -1.0877,  ...,  0.1798, -1.0767,  0.1518],\n",
      "        [-1.3907, -0.3780,  0.4021,  ...,  1.1385,  1.5681, -0.7753],\n",
      "        [ 0.2242,  0.5303,  1.1270,  ..., -1.2575,  1.3673, -0.9312],\n",
      "        ...,\n",
      "        [-0.8702, -0.4270,  0.7792,  ..., -0.6583,  0.8884,  0.1192],\n",
      "        [ 0.2943,  0.7142,  1.9071,  ...,  1.9408,  0.3099,  1.4702],\n",
      "        [ 1.1850,  0.2641, -0.2650,  ..., -0.3974, -0.2148, -0.4283]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(32, 100))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerFFNNEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TwoLayerFFNNEmbeddings, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(3 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        reshaped_embeddings = self.embeddings(X).view((1, -1))\n",
    "        out = F.relu(self.linear1(reshaped_embeddings))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4c] Now, first create your model with the vocabulary size and a embedding dimension of 20. Also choose your favorite loss function and optimizer, and save all to variables. Then using these variables, write a function that will loop over the tokenized dataset and:**<br>\n",
    "- put the indices of the embedding vectors of the following 3 tokens into the <mark>forward</mark> method and calculate log probabilities\n",
    "- calculate the loss by using the log probabilities acquired in the previous step and target word at each iteration, given by your desired loss function\n",
    "- compute the gradient by the backward pass and update the gradient\n",
    "- add the resulting loss to the cumulative loss <br>\n",
    "\n",
    "**in each iteration. In the end run your function to print out the total loss reached by iterating over the whole dataset just once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '''Your code will be here'''\n",
    "loss_fn = '''Your code will be here'''\n",
    "optimizer = '''Your code will be here'''\n",
    "\n",
    "def loss_calculator():\n",
    "    cumulative_loss = 0\n",
    "    for i in range(len(tokenized)-4):\n",
    "        model.zero_grad() # this line of code will refresh the gradients at each step, please keep it!\n",
    "    '''Your code will be here'''\n",
    "    \n",
    "    return cumulative_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[AnswerE4c]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790.557135105133\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerFFNNEmbeddings(len(vocabulary), 20)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def loss_calculator():\n",
    "    cumulative_loss = 0\n",
    "    for i in range(len(tokenized)-4):\n",
    "        model.zero_grad()\n",
    "        log_probs = model.forward(torch.tensor([word_to_idx[w] for w in tokenized[i:i+3]]))\n",
    "        loss = loss_fn(log_probs, torch.tensor([word_to_idx[tokenized[i+4]]]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulative_loss += loss.item()\n",
    "    return cumulative_loss\n",
    "\n",
    "\n",
    "print(loss_calculator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4d] As a final step, write a small script that only runs your loss_calculator function ten consecutive times and prints out the loss. Comment on how the loss value changes at each iteration. What does it signify in terms of model learning?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Your code will be here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[AnswerE4d]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1774.4375233650208\n",
      "1758.5986728668213\n",
      "1743.0114994049072\n",
      "1727.6407690048218\n",
      "1712.4540314674377\n",
      "1697.429223060608\n",
      "1682.5585780143738\n",
      "1667.8570592403412\n",
      "1653.3423690795898\n",
      "1639.0143139362335\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(loss_calculator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we carry out iterations over the whole dataset, the model keeps the learned weights in the embedding matrix and uses that in the next iteration. Therefore it develops an understanding to guess the next word better, judging from the embeddings of the context words coming before that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
