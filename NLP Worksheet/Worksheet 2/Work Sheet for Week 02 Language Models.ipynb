{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Sheet for Week 02: Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions / Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Dear PROF. DR. DAVID SCHLANGEN, I am now learning Python programming and try to solve the worksheets exercise. Sometimes I face difficulties due to the lack of better knowledge on Python. I need to spend more time on reading text and then I go for practice. I dont know how much I accurately solve these problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [E1] Write out the equation for trigram probability estimation (by modifying Eq. 3.11 in JM3, Chapter 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(wn|wn-2,wn-1) = Count(wn-2,wn-1,wn)/Count(wn-2,wn-1)\n"
     ]
    }
   ],
   "source": [
    "print(\"P(wn|wn-2,wn-1) = Count(wn-2,wn-1,wn)/Count(wn-2,wn-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [E2] Write a program to compute unsmoothed unigrams and bigrams from a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need'): 1, ('need', 'to'): 1, ('to', 'write'): 1, ('write', 'a'): 1, ('a', 'program'): 1, ('program', 'in'): 1, ('in', 'NLTK'): 1, ('NLTK', 'that'): 1, ('that', 'breaks'): 1, ('breaks', 'a'): 1, ('a', 'corpus'): 1, ('corpus', '('): 1, ('(', 'a'): 1, ('a', 'large'): 1, ('large', 'collection'): 1, ('collection', 'of'): 1, ('of', 'txt'): 1, ('txt', 'files'): 1, ('files', ')'): 1, (')', 'into'): 1, ('into', 'unigrams'): 1, ('unigrams', ','): 1, (',', 'bigrams'): 1, ('bigrams', ','): 1, (',', 'trigrams'): 1, ('trigrams', ','): 1, (',', 'fourgrams'): 1, ('fourgrams', 'and'): 1, ('and', 'fivegrams'): 1, ('fivegrams', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams.\"\n",
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "print(Counter(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [E3] Run your N-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.', 'Connect', 'connecting', 'connected', 'connects', 'take', 'taking', 'took', 'tooks']\n",
      "<FreqDist with 33 samples and 39 outcomes>\n",
      "Counter({('I', 'need'): 1, ('need', 'to'): 1, ('to', 'write'): 1, ('write', 'a'): 1, ('a', 'program'): 1, ('program', 'in'): 1, ('in', 'NLTK'): 1, ('NLTK', 'that'): 1, ('that', 'breaks'): 1, ('breaks', 'a'): 1, ('a', 'corpus'): 1, ('corpus', '('): 1, ('(', 'a'): 1, ('a', 'large'): 1, ('large', 'collection'): 1, ('collection', 'of'): 1, ('of', 'txt'): 1, ('txt', 'files'): 1, ('files', ')'): 1, (')', 'into'): 1, ('into', 'unigrams'): 1, ('unigrams', ','): 1, (',', 'bigrams'): 1, ('bigrams', ','): 1, (',', 'trigrams'): 1, ('trigrams', ','): 1, (',', 'fourgrams'): 1, ('fourgrams', 'and'): 1, ('and', 'fivegrams'): 1, ('fivegrams', '.'): 1})\n",
      "The Length of Corpa 1: 200\n"
     ]
    }
   ],
   "source": [
    "corpa1=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard. Connect connecting connected connects take taking took tooks \"\"\"\n",
    "tokenized_text = word_tokenize(corpa1)\n",
    "print(tokenized_text)\n",
    "fdist = FreqDist(tokenized_text)\n",
    "print(fdist)\n",
    "bigrams = ngrams(token,2)\n",
    "print(Counter(bigrams))\n",
    "length = len(corpa1)\n",
    "print(\"The Length of Corpa 1: \" + str(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'need', 'to', 'write', 'a', 'program', 'in', 'NLTK', 'that', 'breaks', 'a', 'corpus', '(', 'a', 'large', 'collection', 'of', 'txt', 'files', ')', 'into', 'unigrams', ',', 'bigrams', ',', 'trigrams', ',', 'fourgrams', 'and', 'fivegrams', '.']\n",
      "<FreqDist with 27 samples and 31 outcomes>\n",
      "Counter({('I', 'need'): 1, ('need', 'to'): 1, ('to', 'write'): 1, ('write', 'a'): 1, ('a', 'program'): 1, ('program', 'in'): 1, ('in', 'NLTK'): 1, ('NLTK', 'that'): 1, ('that', 'breaks'): 1, ('breaks', 'a'): 1, ('a', 'corpus'): 1, ('corpus', '('): 1, ('(', 'a'): 1, ('a', 'large'): 1, ('large', 'collection'): 1, ('collection', 'of'): 1, ('of', 'txt'): 1, ('txt', 'files'): 1, ('files', ')'): 1, (')', 'into'): 1, ('into', 'unigrams'): 1, ('unigrams', ','): 1, (',', 'bigrams'): 1, ('bigrams', ','): 1, (',', 'trigrams'): 1, ('trigrams', ','): 1, (',', 'fourgrams'): 1, ('fourgrams', 'and'): 1, ('and', 'fivegrams'): 1, ('fivegrams', '.'): 1})\n",
      "The Length of Corpa 2: 147\n"
     ]
    }
   ],
   "source": [
    "corpa2 = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams.\"\n",
    "tokenized_text = word_tokenize(corpa2)\n",
    "print(tokenized_text)\n",
    "fdist = FreqDist(tokenized_text)\n",
    "print(fdist)\n",
    "token = nltk.word_tokenize(corpa2)\n",
    "bigrams = ngrams(token,2)\n",
    "print(Counter(bigrams))\n",
    "length = len(corpa2)\n",
    "print(\"The Length of Corpa 2: \" + str(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(fileids=['cg22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories=['news', 'editorial', 'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
     ]
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals: \n",
    "    print(m + ':', fdist[m], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 124 could: 57 may: 79 might: 39 must: 55 will: 235 "
     ]
    }
   ],
   "source": [
    "editorial_text = brown.words(categories='editorial')\n",
    "fdist = nltk.FreqDist(w.lower() for w in editorial_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals: \n",
    "    print(m + ':', fdist[m], end=' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [E4] Add an option to your program to generate random sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(nltk.corpus.brown.words()) # Get text from brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is . , purchased by Brown University from Lawrence J. Sullivan , Red\n",
      "Cross Aide Reuben Sleight had been startled to find out if they\n",
      "possessed real military strength , I wasn't surprised , wailed , and\n",
      "there she was rather a nuisance ; ; both were selling at prices that\n",
      "offer the group . and mechanized operation . Af and Af and Af ( F ) In\n",
      "the Cutting Tool Division is to be sure the boat do the work and bore\n",
      "the expenses incurred , you don't look much like the spectrum\n",
      "indicated by the material ( molding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"is . , purchased by Brown University from Lawrence J. Sullivan , Red\\nCross Aide Reuben Sleight had been startled to find out if they\\npossessed real military strength , I wasn't surprised , wailed , and\\nthere she was rather a nuisance ; ; both were selling at prices that\\noffer the group . and mechanized operation . Af and Af and Af ( F ) In\\nthe Cutting Tool Division is to be sure the boat do the work and bore\\nthe expenses incurred , you don't look much like the spectrum\\nindicated by the material ( molding\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.generate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'creature': 7, 'thing': 4, 'substance': 2, 'soul': 1, '.': 1, ',': 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()\n",
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "cfd['living']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "living creature that he said , and the land of the land of the land "
     ]
    }
   ],
   "source": [
    "generate_model(cfd, 'living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [E5] Add an option to compute the probability (according to the model) of an arbitrary input sentence. Create an input sentence that causes a problem. (I.e., where the output is different from your intuition about what the output should be.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to find out a solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [E6] Add Laplace smoothing, and UNK. Does that fix the problem from the previous exercise? What does this intervention do to the probability mass? Compare the smoothed and unsmoothed probability of P(\"the\" | \"end\") and P(\"end\" | \"the\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to find out a solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
