{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Sheet for Week 03: Text Classification & Sentiment Analysis + Logistic Regression\n",
    "\n",
    "## Background\n",
    "\n",
    "In this worksheet, we will follow the material covered either in [Chapter 4 of the JM book](https://web.stanford.edu/~jurafsky/slp3/4.pdf) or in the [Text Classification and Sentiment Analysis Playlist](https://www.youtube.com/playlist?list=PLaZQkZp6WhWxU3kA6wV0nb5dY1SXDEKWH). For the questions about logistic regression, you can refer to [Chapter 5 of the JM book](https://web.stanford.edu/~jurafsky/slp3/5.pdf).\n",
    "\n",
    "Please keep the provided files in your worksheet folder to be able to run the given scripts and see the text examples without any errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E1] You may have noticed that our classification models require a significant number of training examples in order to converge to the underlying probability distribution in the dataset. This brings about the necessity and importance of annotated data: Models require more and more annotated examples each day.**\n",
    "\n",
    "**In this exercise, you'll have the chance to annotate your own small dataset (which is basically taken from [this Kaggle competition](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview) and represented as a json file that includes only 30 examples). Please answer the questions [E1a] and [E1b], after loading the dataset by using the following script:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>r1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>r2</td>\n",
       "      <td>This quiet, introspective and entertaining independent is worth seeking</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>r3</td>\n",
       "      <td>Even fans of Ismail Merchant's work , I suspect, would have a hard time sitting through this one.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>r4</td>\n",
       "      <td>Cattaneo should have followed the runaway success of his first film, The Full Monty, with someth...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>r5</td>\n",
       "      <td>A positively thrilling combination of ethnography and all the intrigue, betrayal, deceit and mur...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id  \\\n",
       "0        r1   \n",
       "1        r2   \n",
       "2        r3   \n",
       "3        r4   \n",
       "4        r5   \n",
       "\n",
       "                                                                                                review  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for t...   \n",
       "1                              This quiet, introspective and entertaining independent is worth seeking   \n",
       "2    Even fans of Ismail Merchant's work , I suspect, would have a hard time sitting through this one.   \n",
       "3  Cattaneo should have followed the runaway success of his first film, The Full Monty, with someth...   \n",
       "4  A positively thrilling combination of ethnography and all the intrigue, betrayal, deceit and mur...   \n",
       "\n",
       "  label  \n",
       "0        \n",
       "1        \n",
       "2        \n",
       "3        \n",
       "4        "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# open JSON file \n",
    "f = open('movie_reviews.json',) \n",
    "  \n",
    "# converts JSON object to dictionary,  and then to a dataframe \n",
    "\n",
    "reviews = json.load(f) \n",
    "\n",
    "movie_reviews_df = pd.DataFrame(reviews['movie_reviews'])\n",
    "\n",
    "movie_reviews_df.style.hide_index() \n",
    "movie_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E1a] Try to annotate each movie review example in the dataframe by only using two class labels: 'Positive' or 'Negative'. Write down your annotations as a list of pairs of ID of review, numerical encoding of label, so that (1,0) might mean \"example 1 is annotated with label 0. You can select 0 as the negative and 1 as the positive label for convenience. Then compare it with the results of your peers. Can you think of a way of quantifying similarities and differences between your annotations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E1b] Imagine doing the task at [E1a] once more, this time having five class labels: 'Positive', 'Somewhat Positive', 'Neutral', 'Somewhat Negative' or 'Negative'. How would you think the differences with respect to your colleagues change? How would you set guidelines for a more robust annotation process, considering also certain arbitration rules for examples with seemingly a lot of different annotation candidates?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E2] In this exercise, you'll be provided with 4 different Turkish newspaper texts: These news belong to 4 different class labels: 'technology', 'sport', 'art and entertainment' and 'economy'.**<br>\n",
    "\n",
    "**Assuming that none of you speak Turkish, how would you assign a unique label to each of these texts given in the dataframe below? Run the script to see the dataframe and think about the appropriate label for each text.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >news_id</th>        <th class=\"col_heading level0 col1\" >review</th>        <th class=\"col_heading level0 col2\" >label</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row0_col0\" class=\"data row0 col0\" >n1</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row0_col1\" class=\"data row0 col1\" >Fitch'in kararÄ±na yurtdÄ±ÅŸÄ±ndan tepkiler: Fitch'in TÃ¼rkiye'nin kredi derecelendirme notunu yÃ¼kseltmesine yurt dÄ±ÅŸÄ±ndaki ekonomistlerden de olumlu tepkiler geldi. UBS geliÅŸmekte olan piyasalar strateji uzmanÄ± Manik Narain, 'Kredi piyasalarÄ± TÃ¼rkiye ile bir sÃ¼redir yatÄ±rÄ±m yapÄ±labilir notu almÄ±ÅŸ gibi ticaret yapÄ±yordu. BugÃ¼nden itibaren reel bir yatÄ±rÄ±m akÄ±ÅŸÄ± baÅŸlayacak ve kredi masraflarÄ± dÃ¼ÅŸecek' dedi.Â Standart Bank geliÅŸmekte olan piyasalar araÅŸtÄ±rma mÃ¼dÃ¼rÃ¼ Timothy Ash ise 'YatÄ±rÄ±m yapÄ±labilir notu TÃ¼rkiye'ye uzun zaman Ã¶nce verilmeliydi' diye konuÅŸurken Ã¼lkeye dayatÄ±lan dÄ±ÅŸ finansal risklerin uzun sÃ¼redir abartÄ±ldÄ±ÄŸÄ±nÄ± belirtti. Ash, bu durumun TÃ¼rkiye'ye gÃ¼veni arttÄ±racaÄŸÄ±nÄ±, Moody ve S&P'nin de Fitch'i takip etmesini beklediÄŸini sÃ¶zlerine ekledi.</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row0_col2\" class=\"data row0 col2\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row1_col0\" class=\"data row1 col0\" >n2</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row1_col1\" class=\"data row1 col1\" >Herkes Burak'a Murat Burak'a. BÃ¼yÃ¼kÅŸehir maÃ§Ä±nda en Ã§ok pas '43'le Burak YÄ±lmaz'a atÄ±ldÄ±. G.Saray'daki en iyi futbolunu oynayan Kral da kaleye sÄ±rtÄ± dÃ¶nÃ¼k aldÄ±ÄŸÄ± paslarÄ±, Umut'a asiste Ã§evirdi. G.Saray Teknik DirektÃ¶rÃ¼ Fatih Terim, Cluj sÄ±navÄ±nÄ±n provasÄ±nÄ± yaptÄ±ÄŸÄ± BÃ¼yÃ¼kÅŸehir maÃ§Ä±nda taktik zekasÄ±ndan Ã¶rnekler sergiledi. Fatih Hoca, '3 pasta gol' atma dersi verdi. Cim Bom'un 10 gol giriÅŸiminde ortalama pas sayÄ±sÄ± 3.20 oldu. 3 pasta kaleye gitme taktiÄŸinin odak noktasÄ±ndaki isimse Burak YÄ±lmaz'dÄ±.</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row1_col2\" class=\"data row1 col2\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row2_col0\" class=\"data row2 col0\" >n3</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row2_col1\" class=\"data row2 col1\" >Bayramda siber suÃ§ kurbanÄ± olmamanÄ±n 7 yolu: DÃ¼nyanÄ±n en bÃ¼yÃ¼k antivirÃ¼s yazÄ±lÄ±m kuruluÅŸlarÄ±ndan ESET, TÃ¼rkiye'de Kurban BayramÄ± dÃ¶neminde seyahatlerin ve kiÅŸisel bilgisayar kullanÄ±mÄ±nÄ±n artmasÄ± nedeniyle kullanÄ±cÄ±larÄ±n siber suÃ§ kurbanÄ± olmamasÄ± iÃ§in uyarÄ±larda bulundu. 'USB gibi taÅŸÄ±nabilir hafÄ±za kartlarÄ±nÄ±n bÃ¶yle dÃ¶nemlerde paylaÅŸÄ±mÄ± artÄ±yor' diyen ESET TÃ¼rkiye Genel MÃ¼dÃ¼r YardÄ±mcÄ±sÄ± Alev Akkoyunlu, 'KontrolsÃ¼z kullanÄ±lan bellekler gÃ¼nÃ¼mÃ¼zÃ¼n en bÃ¼yÃ¼k virÃ¼s bulaÅŸtÄ±rÄ±cÄ±larÄ±' aÃ§Ä±klamasÄ±nÄ± yaptÄ±. Akkoyunlu, bu dÃ¶nemde internette bayram armaÄŸanÄ± olarak ulaÅŸan 'imkansÄ±z' teklifler ve mesajlar konusunda da uyanÄ±k olmaya Ã§aÄŸÄ±rarak bayramda gÃ¼vende kalmak iÃ§in 7 Ã¶neride bulundu.</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row2_col2\" class=\"data row2 col2\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row3_col0\" class=\"data row3 col0\" >n4</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row3_col1\" class=\"data row3 col1\" >Stok, 17 Ekim'de Sangria Live'da BaÄŸdat Caddesiâ€™nin tek rock barÄ± olan Sangria, sizi genÃ§ ve enerjik bir grupla eÄŸlenceye davet ediyor. 17 Ekim Cumartesi akÅŸamÄ± Stok en yeni ve eÄŸlenceli repertuvarÄ± ile sahnede olacak. Bir yerli iÃ§ki dahil 25 TL olan konser hafta ortasÄ± keyfinizi yerine getirecek. 13 yÄ±ldÄ±r rock severlerin BaÄŸdat Caddesiâ€™ndeki favori mekanÄ± olan Sangria Liveâ€™da konserler son hÄ±zÄ±yla devam ediyor.</td>\n",
       "                        <td id=\"T_28492c8a_29bc_11eb_b98b_5ce0c5253cf2row3_col2\" class=\"data row3 col2\" ></td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2472c4ce188>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# open JSON file \n",
    "f = open('turkish_news_examples.json',) \n",
    "  \n",
    "# converts JSON object to dictionary,  and then to a dataframe \n",
    "\n",
    "news = json.load(f) \n",
    "\n",
    "turkish_news_df = pd.DataFrame(news['turkish_news'])\n",
    "\n",
    "turkish_news_df.at[0, 'label'] = \"\"\n",
    "turkish_news_df.at[1, 'label'] = \"\"\n",
    "turkish_news_df.at[2, 'label'] = \"\"\n",
    "turkish_news_df.at[3, 'label'] = \"\"\n",
    "\n",
    "\n",
    "turkish_news_df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you make your predictions? Do you look at any particular words to decide on the actual label? How does the way that you solved this task relate to how Naive Bayes does classification?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E3] If the document is represented by a certain type of feature, there is a direct relationship between a Naive Bayes classifier and class conditional unigram models. How? Did you use such a relationship while solving [E2]?**<br>\n",
    "\n",
    "**In other words, you need to consider a given text document represented by a certain type of feature (e.g. words themselves, parts of speech of words, pronunciations etc.), and our text classification model works with the mere counts of each type in the feature vocabulary. Would such a model resemble to what we have experienced so far with unigram models?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E4] What's the problem with correlated features for NB, and how may those occur in natural language?<br>\n",
    "**\n",
    "Hint: If you are not sure about how correlated features effect the performance of NB classifiers, you can check out this [link](https://machinelearningmastery.com/better-naive-bayes/) to get some ideas about how to design an NB algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E5] Derive the gradient for binary logistic regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[E6] Derive the gradient for multi-class logistic regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
